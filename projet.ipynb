{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des librairies nécessaire pour le programe\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.5\n",
      "out.mp4 file created\n"
     ]
    }
   ],
   "source": [
    "#mettre le chemin de la vidéo\n",
    "#ce fichier stabilise une vidéo en prenant un point fixe\n",
    "#source : eva \n",
    "%run ./single_object_tracker.py ./test_final/seq_5.mp4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si érreur : module 'cv2' has no attribute 'legacy', exécuter les lignes suivantes\n",
    "#pip uninstall opencv-python\n",
    "#pip uninstall opencv-contrib-python\n",
    "#pip uninstall opencv-contrib-python-headless\n",
    "#pip install opencv-contrib-python=4.5.5.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "videoPath = \"out.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation de la liste des objets que l'on peut détécter : liste coco\n",
    "classFile = \"coco.names.txt\"\n",
    "\n",
    "with open(classFile, 'r') as f:\n",
    "    names = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation du model entrainé pour détécter les objet. On utilise ici le faster rcnn \n",
    "model_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet152_v1_800x1333_coco17_gpu-8.tar.gz'\n",
    "\n",
    "fileName = os.path.basename(model_url)\n",
    "Name = fileName[:fileName.index('.')]\n",
    "\n",
    "cacheDir = './pretrained_models'\n",
    "os.makedirs(cacheDir, exist_ok=True)\n",
    "\n",
    "get_file(fname=fileName,\n",
    "        origin=model_url,\n",
    "        cache_dir=cacheDir,\n",
    "        cache_subdir=\"checkpoints\",\n",
    "        extract=True)\n",
    "\n",
    "model = tf.saved_model.load(os.path.join(cacheDir, \"checkpoints\", Name, \"saved_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction qui permet d'utiliser le modèle chargé précédement sur une image\n",
    "def createBoundingBox(image):\n",
    "    colorList = np.random.uniform(low=0, high=255, size=(len(names), 3))\n",
    "    \n",
    "    inputTensor = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2RGB)\n",
    "    inputTensor = tf.convert_to_tensor(inputTensor, dtype=tf.uint8)\n",
    "    inputTensor = inputTensor[tf.newaxis,...]\n",
    "    \n",
    "    detection = model(inputTensor)\n",
    "    \n",
    "    bboxs = detection['detection_boxes'][0].numpy()\n",
    "    Indexes = detection['detection_classes'][0].numpy().astype(np.int32)\n",
    "    Scores = detection['detection_scores'][0].numpy()\n",
    "    \n",
    "    imH, imW, imC = image.shape\n",
    "    \n",
    "    bboxIdx = tf.image.non_max_suppression(bboxs, \n",
    "                                           Scores, \n",
    "                                           max_output_size=50,\n",
    "                                          iou_threshold=threshold,\n",
    "                                          score_threshold=threshold)\n",
    "    \n",
    "    \n",
    "    if len(bboxIdx) != 0:\n",
    "        for i in bboxIdx:\n",
    "            bbox = tuple(bboxs[i].tolist())\n",
    "            Confidence = round(100*Scores[i])\n",
    "            Index = Indexes[i]\n",
    "            \n",
    "            LabelText = names[Index-1].upper()\n",
    "            Color = colorList[Index]\n",
    "            \n",
    "            \n",
    "            \n",
    "            ymin, xmin, ymax, xmax = bbox\n",
    "            xmin, xmax, ymin, ymax = (xmin*imW, xmax*imW, ymin*imH, ymax*imH)\n",
    "            xmin, xmax, ymin, ymax = int(xmin), int(xmax), int(ymin), int(ymax)\n",
    "            \n",
    "            displayText = '{}: {}%'.format(LabelText, Confidence)\n",
    "            \n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax,ymax), color=Color, thickness=1)\n",
    "            cv2.putText(image, displayText, (xmin, ymin-10), cv2.FONT_HERSHEY_PLAIN, 1, Color, 2)\n",
    "            \n",
    "            #lineWidth = min(int((xmax-xmin)*0.2), int((ymax-ymin)*0.2))\n",
    "            \n",
    "            #cv2.line(image, (xmin,ymin), (xmin + lineWidth, ymin), Color, thickness=5)\n",
    "            \n",
    "            print(LabelText)\n",
    "            print('Taille', xmax-xmin, ymax-ymin)\n",
    "            #print('Positon : ',  xmin + (xmax-xmin)/2, ymin+(ymin-ymax)/2)\n",
    "            print()\n",
    "            \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'\\'\\n#detect objects in a picture\\n\\nimagePath = \"./test/test_stop.jpg\"\\n\\nimage = cv2.imread(imagePath) \\nbbox_image = createBoundingBox(image)\\ncv2.imshow(\"Result\", bbox_image)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''''\n",
    "#detect objects in a picture\n",
    "\n",
    "imagePath = \"./test/test_stop.jpg\"\n",
    "\n",
    "image = cv2.imread(imagePath) \n",
    "bbox_image = createBoundingBox(image)\n",
    "cv2.imshow(\"Result\", bbox_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "'''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAR\n",
      "Taille 71 44\n",
      "\n",
      "TRAIN\n",
      "Taille 281 193\n",
      "\n",
      "TRAIN\n",
      "Taille 272 265\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 59 99\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 58 98\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 59 100\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 59 97\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 62 99\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 60 100\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 63 95\n",
      "\n",
      "UMBRELLA\n",
      "Taille 97 39\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 64 96\n",
      "\n",
      "BOTTLE\n",
      "Taille 23 43\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 62 101\n",
      "\n",
      "BIRD\n",
      "Taille 176 169\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 39 80\n",
      "\n",
      "POTTED PLANT\n",
      "Taille 40 76\n",
      "\n",
      "BIRD\n",
      "Taille 175 183\n",
      "\n",
      "AIRPLANE\n",
      "Taille 85 76\n",
      "\n",
      "PERSON\n",
      "Taille 15 34\n",
      "\n",
      "PERSON\n",
      "Taille 14 34\n",
      "\n",
      "PERSON\n",
      "Taille 17 35\n",
      "\n",
      "PERSON\n",
      "Taille 15 36\n",
      "\n",
      "PERSON\n",
      "Taille 15 34\n",
      "\n",
      "PERSON\n",
      "Taille 17 36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#detect objects in a video using fonction createBoundingBox\n",
    "cap = cv2.VideoCapture(videoPath)\n",
    "\n",
    "if(cap.isOpened() == False):\n",
    "    print(\"Error opening file ...\")\n",
    "\n",
    "success, image = cap.read()\n",
    "\n",
    "startTime = 0\n",
    "\n",
    "while success:\n",
    "    currentTime = time.time()\n",
    "    \n",
    "    fps = 1/(currentTime - startTime)\n",
    "    startTime = currentTime  \n",
    "    \n",
    "    bbox_Image = createBoundingBox(image)\n",
    "    \n",
    "    cv2.putText(bbox_Image, \"FPS: \" + str(int(fps)), (20,70), cv2.FONT_HERSHEY_PLAIN, 2, (0,255,0), 2)\n",
    "    cv2.imshow(\"Result\", bbox_Image)\n",
    "    \n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "    if key == ord(\"e\"):\n",
    "        break\n",
    "    \n",
    "    success, image = cap.read()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rentrer le nom et les valeurs Taille et Position d'un objet proche de l'eau\n",
    "#pour seq_5 personne à la fin de la vidéo avec une taille de 13,36\n",
    "Name = \"person\"\n",
    "Taille = [17, 36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting segmentation\n",
      "FPS: 30.0\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "#utilisation de ce programme pour extraire une image du programe\n",
    "%run ./OF-segmentation.py out.mp4 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#seq_5 personne: 500; 122\\n\\nArrayBlanc = np.load('ArrayBlanc.npy')\\n\\ncoefx = 640*20/100\\ncenterx = 279\\nxmin, xmax = centerx-coefx, centerx+coefx\\n\\ncoefy = 360*20/100\\ncentery = 214\\nymin, ymax = centery-coefy, centery+coefy\\n\\n\\ncleanArray = []\\nfor points in ArrayBlanc:\\n    x, y, z = points\\n    if x>xmin and x<xmax:\\n        if y>ymin and y<ymax:\\n            cleanArray.append([x,y,z])\\ncleanArray\\nArrayBlanc.shape\\n\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#seq_5 personne: 500; 122\n",
    "\n",
    "ArrayBlanc = np.load('ArrayBlanc.npy')\n",
    "\n",
    "coefx = 640*20/100\n",
    "centerx = 279\n",
    "xmin, xmax = centerx-coefx, centerx+coefx\n",
    "\n",
    "coefy = 360*20/100\n",
    "centery = 214\n",
    "ymin, ymax = centery-coefy, centery+coefy\n",
    "\n",
    "\n",
    "cleanArray = []\n",
    "for points in ArrayBlanc:\n",
    "    x, y, z = points\n",
    "    if x>xmin and x<xmax:\n",
    "        if y>ymin and y<ymax:\n",
    "            cleanArray.append([x,y,z])\n",
    "cleanArray\n",
    "ArrayBlanc.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322   220\n",
      "375   220\n",
      "376   237\n",
      "321   238\n"
     ]
    }
   ],
   "source": [
    "#choisir quatre points dans l'eau, au plus proche de l'objet et appuyer sur 'e'\n",
    "#extraction du périmètre où l'on va regarder les vecteurs de vitesse\n",
    "%run ./position.py ./out_frame.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "xArray = []\n",
    "yArray = []\n",
    "\n",
    "with open(\"./coordonnees_eau.txt\") as f:\n",
    "    for line in f:\n",
    "        x1, y1 = line.split(' ')\n",
    "        xArray.append(int(x1))\n",
    "        yArray.append(int(y1.split('\\n')[0]))\n",
    "        \n",
    "xmin, xmax = min(xArray), max(xArray)\n",
    "ymin, ymax = min(yArray), max(yArray)\n",
    "\n",
    "waterArray = []\n",
    "\n",
    "y = ymin\n",
    "while y < ymax:\n",
    "    i = xmin\n",
    "    while i < xmax:\n",
    "        waterArray.append([y,i])\n",
    "        i += 1\n",
    "    y +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_flow(img, flow, step=16):\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2,-1).astype(int)\n",
    "    fx, fy = flow[y,x].T\n",
    "    lines = np.vstack([x, y, x-fx, y-fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "\n",
    "    img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.polylines(img_bgr, lines, 0, (0, 255, 0))\n",
    "\n",
    "    for (x1, y1), (_x2, _y2) in lines:\n",
    "        cv2.circle(img_bgr, (x1, y1), 1, (0, 255, 0), -1)\n",
    "\n",
    "    return img_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def draw_hsv(flow):\n",
    "\n",
    "#    h, w = flow.shape[:2]\n",
    "#    fx, fy = flow[:,:,0], flow[:,:,1]\n",
    "#    ang = np.arctan2(fy, fx) + np.pi\n",
    "#    v = np.sqrt(fx*fx+fy*fy)\n",
    "\n",
    "#    hsv = np.zeros((h, w, 3), np.uint8)\n",
    "#    hsv[...,0] = ang*(180/np.pi/2)\n",
    "#    hsv[...,1] = 255\n",
    "#    hsv[...,2] = np.minimum(v*4, 255)\n",
    "#    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "#    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trouve des vecteurs de vitesse\n",
    "\n",
    "cap = cv2.VideoCapture(videoPath)\n",
    "\n",
    "suc, prev = cap.read()\n",
    "prevgray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "suc, img = cap.read()\n",
    "\n",
    "cleanfx, cleanfy = [], []\n",
    "cfx, cfy = [], []\n",
    "\n",
    "while suc:\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # start time to calculate FPS\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prevgray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    fx, fy = flow[:,:,0], flow[:,:,1]\n",
    "    for i in range(len(waterArray)):\n",
    "        cleanfx.append(fx[waterArray[i][0], waterArray[i][1]])\n",
    "        cleanfy.append(fy[waterArray[i][0], waterArray[i][1]])\n",
    "    #flow = cv2.calcOpticalFlowFarneback(prevgray, gray, flow=None, pyr_scale=0.5, levels=3, winsize=20, iterations=15, poly_n=5, poly_sigma=1.2, flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "    \n",
    "    cfx.append(np.mean(np.abs(cleanfx)))\n",
    "    cfy.append(np.mean(np.abs(cleanfy)))\n",
    "    \n",
    "    prevgray = gray\n",
    "\n",
    "\n",
    "    # End time\n",
    "    end = time.time()\n",
    "    # calculate the FPS for current frame detection\n",
    "    fps = 1 / (end-start)\n",
    "\n",
    "    #print(f\"{fps:.2f} FPS\")\n",
    "    \n",
    "    img_bgr = draw_flow(gray, flow)\n",
    "    \n",
    "    cv2.imshow('flow', img_bgr)\n",
    "    #cv2.imshow('flow HSV', draw_hsv(flow))\n",
    "    \n",
    "    key = cv2.waitKey(5)\n",
    "    if key == ord('e'):\n",
    "        break\n",
    "    \n",
    "    suc, img = cap.read()\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "vitx = np.mean(cfx)\n",
    "vity = np.mean(cfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration in seconds: 9\n",
      "video time: 0:00:09\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import datetime \n",
    "  \n",
    "data = cv2.VideoCapture(videoPath) \n",
    "  \n",
    "frames = data.get(cv2.CAP_PROP_FRAME_COUNT) \n",
    "fps = int(data.get(cv2.CAP_PROP_FPS)) \n",
    "  \n",
    "seconds = int(frames / fps) \n",
    "video_time = str(datetime.timedelta(seconds=seconds)) \n",
    "print(\"duration in seconds:\", seconds) \n",
    "print(\"video time:\", video_time) \n",
    "print(fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la vitesse de l'eau sur cette video est  3.104975682879396 m/s\n",
      "la vitesse de l'eau sur cette video est  11.177912458365826 km/h\n"
     ]
    }
   ],
   "source": [
    "#calcul de la vitesse\n",
    "\n",
    "with open(\"./value_coco.txt\") as f:\n",
    "    for line in f:\n",
    "        if Name in line:\n",
    "            n, x, y, z = line.split(' ')\n",
    "            break\n",
    "\n",
    "x, y = int(x), int(y)\n",
    "\n",
    "ex = x/Taille[0]\n",
    "ey = y/Taille[0]\n",
    "\n",
    "vx = vitx*ex\n",
    "vy = vity*ey\n",
    "\n",
    "vitesse_eau = np.sqrt(vx**2 + vy**2) * 0.01 * fps\n",
    "print(\"la vitesse de l'eau sur cette video est \", vitesse_eau, 'm/s')\n",
    "vitesse_eau *= 3.6\n",
    "print(\"la vitesse de l'eau sur cette video est \", vitesse_eau, 'km/h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
